{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c870ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check for precedents\n",
    "import json\n",
    "with open('jsonformatter.txt', 'r') as f:\n",
    "    with open(\"Precedents output.txt\", 'w') as out:\n",
    "        data = json.load(f)\n",
    "\n",
    "        # print(type(data['label']))\n",
    "        for case in data:\n",
    "            out.write('Case id : ' + str(case['id']) + '\\n')\n",
    "            annotation = case['annotations'][0]\n",
    "            # print(annotation['result'])\n",
    "            for key in annotation['result']:\n",
    "                # out.write(key['value']['labels'][0] + '\\n')\n",
    "                if key['value']['labels'][0] == 'STA':\n",
    "                    out.write('STA : ' + key['value']['text'] + '\\n\\n')\n",
    "\n",
    "            out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check for precedents relied\n",
    "import json\n",
    "with open('jsonformatter.txt', 'r') as f:\n",
    "    with open(\"Precedents relied output.txt\", 'w') as out:\n",
    "        data = json.load(f)\n",
    "\n",
    "        for case in data:\n",
    "            out.write('Id : ' + str(case['id']) + '\\n')\n",
    "            annotation = case['annotations'][0]\n",
    "            for key in annotation['result']:\n",
    "                if key['value']['labels'][0] == 'PRE_RELIED':\n",
    "                    out.write('PRE_RELIED : ' + key['value']['text'] + '\\n\\n')\n",
    "\n",
    "            out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea61b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check for precedents not relied\n",
    "import json\n",
    "with open('jsonformatter.txt', 'r') as f:\n",
    "    with open(\"Precedents not relied output.txt\", 'w') as out:\n",
    "        data = json.load(f)\n",
    "\n",
    "        for case in data:\n",
    "            out.write('Id : ' + str(case['id']) + '\\n')\n",
    "            annotation = case['annotations'][0]\n",
    "            for key in annotation['result']:\n",
    "                if key['value']['labels'][0] == 'PRE_NOT_RELIED':\n",
    "                    out.write('PRE_NOT_RELIED : ' + key['value']['text'] + '\\n\\n')\n",
    "\n",
    "            out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42899f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To seperate the precedents on the keyword \"vs\" or \"v.\"\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "directories = ['Writ Petition', 'Civil Appeal']\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        mytree = ET.ElementTree(file= f)\n",
    "        myroot = mytree.getroot()\n",
    "\n",
    "        out_dir = directory+' precedents output'\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "        with open( out_file, 'w') as out:\n",
    "            for i in myroot:\n",
    "                if(i.tag=='JudgmentText'):\n",
    "                    for legis in i:\n",
    "                        # out.write(\"Inside Judgement Text : \" + legis.tag + '\\n')\n",
    "                        if(legis.tag=='I'):\n",
    "                            for para in legis:\n",
    "                                if para.text is None:\n",
    "                                    continue\n",
    "                                s1 = para.text.lower()\n",
    "                                s2 = para.text\n",
    "                                if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                                    out.write(\"Precedent : \" + para.text + '\\n\\n')\n",
    "\n",
    "                        elif(legis.tag=='P'):\n",
    "\n",
    "                            if legis.text is None:\n",
    "                                continue\n",
    "\n",
    "                            s1 = legis.text.lower()\n",
    "                            s2 = legis.text\n",
    "                            if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                                out.write(\"Precedent : \" + legis.text + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9769d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To classify precedents\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "directories = ['Writ Petition', 'Civil Appeal']\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        mytree = ET.ElementTree(file= f)\n",
    "        myroot = mytree.getroot()\n",
    "\n",
    "        out_dir = directory+' precedents classified output'\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "        with open( out_file, 'w') as out:\n",
    "            for i in myroot:\n",
    "                if(i.tag=='JudgmentText'):\n",
    "                    for legis in i:\n",
    "                        if(legis.tag=='I'):\n",
    "                            for para in legis:\n",
    "                                if para.text is None:\n",
    "                                    continue\n",
    "                                s1 = para.text.lower()\n",
    "                                s2 = para.text\n",
    "                                if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                                    if 'overruled' in s1:\n",
    "                                        out.write(\"Precedent Overruled : \")\n",
    "                                    elif 'distinguish' in s1:\n",
    "                                        out.write(\"Precedent Distinguished : \")\n",
    "                                    elif sia.polarity_scores(s2)[\"compound\"] > 0:\n",
    "                                        out.write(\"Precedent Relied : \")\n",
    "                                    else:\n",
    "                                        out.write(\"Precedent Referred : \")\n",
    "                                    out.write(s2 + '\\n\\n')\n",
    "\n",
    "                        elif(legis.tag=='P'):\n",
    "\n",
    "                            if legis.text is None:\n",
    "                                continue\n",
    "\n",
    "                            s1 = legis.text.lower()\n",
    "                            s2 = legis.text\n",
    "                            if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                                if 'overruled' in s1:\n",
    "                                    out.write(\"Precedent Overruled : \")\n",
    "                                elif 'distinguish' in s1:\n",
    "                                    out.write(\"Precedent Distinguished : \")\n",
    "                                elif sia.polarity_scores(s2)[\"compound\"] > 0:\n",
    "                                    out.write(\"Precedent Relied : \")\n",
    "                                else:\n",
    "                                    out.write(\"Precedent Referred : \")\n",
    "                                out.write(s2 + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9224c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence wise classification of precedents\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "directories = ['Writ Petition', 'Civil Appeal']\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        mytree = ET.ElementTree(file= f)\n",
    "        myroot = mytree.getroot()\n",
    "\n",
    "        out_dir = directory+' precedents classified output'\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "        with open( out_file, 'w') as out:\n",
    "            for i in myroot:\n",
    "                if(i.tag=='JudgmentText'):\n",
    "                    for legis in i:\n",
    "                        if(legis.tag=='I'):\n",
    "                            for para in legis:\n",
    "                                if para.text is None:\n",
    "                                    continue\n",
    "                                flag = 0\n",
    "                                s = \"\"\n",
    "                                for sent in sent_tokenize(para.text):\n",
    "                                    s1 = sent.lower()\n",
    "                                    s2 = sent\n",
    "                                    if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                                        flag=1\n",
    "                                        s += s2\n",
    "                                if flag:\n",
    "                                    if 'overruled' in s.lower():\n",
    "                                        out.write(\"Precedent Overruled : \")\n",
    "                                    elif 'distinguish' in s.lower():\n",
    "                                        out.write(\"Precedent Distinguished : \")\n",
    "                                    elif sia.polarity_scores(s2)[\"compound\"] > 0:\n",
    "                                        out.write(\"Precedent Relied : \")\n",
    "                                    else:\n",
    "                                        out.write(\"Precedent Referred : \")\n",
    "                                    out.write(s + '\\n\\n')\n",
    "\n",
    "                        elif(legis.tag=='P'):\n",
    "\n",
    "                            if legis.text is None:\n",
    "                                continue\n",
    "\n",
    "                            flag = 0\n",
    "                            s = \"\"\n",
    "                            for sent in sent_tokenize(legis.text):\n",
    "                                s1 = sent.lower()\n",
    "                                s2 = sent\n",
    "                                if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                                    flag=1\n",
    "                                    s += s2\n",
    "                            if flag:\n",
    "                                if 'overruled' in s.lower():\n",
    "                                    out.write(\"Precedent Overruled : \")\n",
    "                                elif 'distinguish' in s.lower():\n",
    "                                    out.write(\"Precedent Distinguished : \")\n",
    "                                elif sia.polarity_scores(s2)[\"compound\"] > 0:\n",
    "                                    out.write(\"Precedent Relied : \")\n",
    "                                else:\n",
    "                                    out.write(\"Precedent Referred : \")\n",
    "                                out.write(s + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c078b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To differentiate between capital V and small v\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "with open( 'Precedents output.txt', 'w') as out:\n",
    "    directories = ['Writ Petition', 'Civil Appeal']\n",
    "    for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "            f = os.path.join(directory, filename)\n",
    "            mytree = ET.ElementTree(file= f)\n",
    "            myroot = mytree.getroot()\n",
    "\n",
    "            for i in myroot:\n",
    "                if(i.tag=='JudgmentText'):\n",
    "                    for legis in i:\n",
    "                        # out.write(\"Inside Judgement Text : \" + legis.tag + '\\n')\n",
    "                        if(legis.tag=='I'):\n",
    "                            for para in legis:\n",
    "                                if para.text is None:\n",
    "                                    continue\n",
    "                                s1 = para.text.lower()\n",
    "                                s2 = para.text\n",
    "                                if 'overrule' in s1: # or (' v ' in s1 and ' v ' not in s2): \n",
    "                                    out.write(directory + ' : ' + filename  + ' : ' + \"Precedent : \" + para.text + '\\n\\n')\n",
    "\n",
    "                        elif(legis.tag=='P'):\n",
    "\n",
    "                            if legis.text is None:\n",
    "                                continue\n",
    "\n",
    "                            \n",
    "                            s1 = legis.text.lower()\n",
    "                            s2 = legis.text\n",
    "                            if 'overrule' in s1: # or (' v ' in s1 and ' v ' not in s2): \n",
    "                                out.write(directory + ' : '  + filename + ' : '  + \"Precedent : \" + legis.text + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7681067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence wise classification of precedents in pdf format\n",
    "\n",
    "import PyPDF2\n",
    "import os\n",
    "# import nltk.data\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "directories = ['New cases']\n",
    "for directory in directories:\n",
    "    out_dir = directory+' precedents classified output'\n",
    "    for filename in os.listdir(directory):\n",
    "        # f = os.path.join(directory, filename)\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".pdf\", \" output.txt\"))\n",
    "        out_file.encode('utf-8')\n",
    "        with open(\"{}/{}\".format(directory,filename), 'r', encoding = 'utf-8') as pdfFileObj:\n",
    "            object = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "            NumPages = object.getNumPages()\n",
    "            with open( out_file, 'w') as out:\n",
    "                for i in range(NumPages):\n",
    "                    Text = object.getPage(i).extractText()\n",
    "                    flag = 0\n",
    "                    s = \"\"\n",
    "                    for sent in sent_tokenize(Text):\n",
    "                        s1 = sent.lower()\n",
    "                        s2 = sent\n",
    "                        if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                            flag=1\n",
    "                            s += s2\n",
    "                    if flag:\n",
    "                        if 'overruled' in s.lower():\n",
    "                            out.write(\"Precedent Overruled : \")\n",
    "                        elif 'distinguish' in s.lower():\n",
    "                            out.write(\"Precedent Distinguished : \")\n",
    "                        elif sia.polarity_scores(s2)[\"compound\"] > 0:\n",
    "                            out.write(\"Precedent Relied : \")\n",
    "                        else:\n",
    "                            out.write(\"Precedent Referred : \")\n",
    "                        out.write(s + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f20bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence wise classification of precedents\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "# import nltk.data\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "directories = ['Annotated Data_with xml and original PDF']\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        if '.xml' in filename:\n",
    "            f = os.path.join(directory, filename)\n",
    "            mytree = ET.parse(f)\n",
    "            # myroot = mytree.getroot()\n",
    "\n",
    "            out_dir = directory+' precedents classified output'\n",
    "            out_file = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "            with open( out_file, 'w') as out:\n",
    "                for i in mytree.iter():\n",
    "                    if i.text is not None:\n",
    "                        out.write(i.text + '\\n')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd0a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence wise classification of precedents from txt file for 26 cases\n",
    "\n",
    "import os\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "directories = ['Twenty_six']\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        out_dir = directory+' output'\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".txt\", \" output.txt\"))\n",
    "        with open(f, 'r') as in_file:\n",
    "            with open( out_file, 'w') as out:\n",
    "                for s2 in in_file:\n",
    "                    # out.write(line + '\\n')\n",
    "                    flag = 0\n",
    "                    s1 = s2.lower()\n",
    "                    if ' vs' in s1 or ' v. ' in s2 or ' v ' in s2:\n",
    "                        flag=1\n",
    "                    if flag:\n",
    "                        if 'overruled' in s1.lower():\n",
    "                            out.write(\"Precedent Overruled : \")\n",
    "                        elif 'distinguish' in s1.lower():\n",
    "                            out.write(\"Precedent Distinguished : \")\n",
    "                        elif sia.polarity_scores(s2)[\"compound\"] > 0:\n",
    "                            out.write(\"Precedent Relied : \")\n",
    "                        else:\n",
    "                            out.write(\"Precedent Referred : \")\n",
    "                        out.write(s1 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to extract and classify precedents from any XML file with text within JudgementText\n",
    "import os\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "directories = ['Annotated Data_with xml and original PDF']\n",
    "for directory in directories:\n",
    "    out_dir = directory+' output'\n",
    "    for filename in os.listdir(directory):\n",
    "        if '.xml' in filename:\n",
    "            f = os.path.join(directory, filename)\n",
    "            out_file = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "            with open(f, 'r') as file:\n",
    "                with open( out_file, 'w') as out:\n",
    "                    plain_text = \"\"\n",
    "                    for line in file:\n",
    "                        plain_text += line\n",
    "                    sub1 = '<JudgmentText>'\n",
    "                    sub2 = '</JudgmentText>'\n",
    "                    idx1 = plain_text.index(sub1) + len(sub1)\n",
    "                    idx2 = plain_text.index(sub2)\n",
    "                    text = \"\"\n",
    "                    flag=1\n",
    "                    for i in range(idx1, idx2):\n",
    "                        char = plain_text[i]\n",
    "                        if char == '<':\n",
    "                            flag=0\n",
    "                        elif char == '>':\n",
    "                            flag=1\n",
    "                        elif flag:\n",
    "                            # if char != '\\n':# or text[-1] == '.':\n",
    "                            text += char\n",
    "                    text = \" \".join(text.split())\n",
    "                    # out.write(text + '\\n\\n')\n",
    "                    sentences = sent_tokenize(text)\n",
    "                    flag=0\n",
    "                    prnt_snt = \"\"\n",
    "                    for sent in sentences:\n",
    "                        # out.write(sent + '\\n\\n')\n",
    "                        s1 = sent.lower()\n",
    "                        if ' vs' in s1 or ' v. ' in sent or ' v ' in sent:\n",
    "                            flag = 1\n",
    "                            prnt_snt = sent\n",
    "                        elif flag:\n",
    "                            prnt_snt += sent\n",
    "                            s1 = prnt_snt.lower()\n",
    "                            if 'overruled' in s1:\n",
    "                                out.write(\"Precedent Overruled : \")\n",
    "                            elif 'distinguish' in s1:\n",
    "                                out.write(\"Precedent Distinguished : \")\n",
    "                            elif sia.polarity_scores(prnt_snt)[\"compound\"] > 0:\n",
    "                                out.write(\"Precedent Relied : \")\n",
    "                            else:\n",
    "                                out.write(\"Precedent Referred : \")\n",
    "                            out.write(prnt_snt + '\\n\\n')\n",
    "                            flag = 0\n",
    "                            prnt_snt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to extract and classify precedents from any XML file with text within JudgementText\n",
    "import os\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "# import en_core_web_sm\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "spacy.load('en_core_web_sm')\n",
    "\n",
    "directories = ['Annotated Data_with xml and original PDF']\n",
    "for directory in directories:\n",
    "    out_dir = directory+' output'\n",
    "    for filename in os.listdir(directory):\n",
    "        if '.xml' in filename:\n",
    "            f = os.path.join(directory, filename)\n",
    "            out_file = os.path.join(out_dir, filename.replace(\".xml\", \" output.txt\"))\n",
    "            with open(f, 'r') as file:\n",
    "                with open( out_file, 'w') as out:\n",
    "                    plain_text = \"\"\n",
    "                    for line in file:\n",
    "                        plain_text += line\n",
    "                    sub1 = '<JudgmentText>'\n",
    "                    sub2 = '</JudgmentText>'\n",
    "                    idx1 = plain_text.index(sub1) + len(sub1)\n",
    "                    idx2 = plain_text.index(sub2)\n",
    "                    text = \"\"\n",
    "                    flag=1\n",
    "                    for i in range(idx1, idx2):\n",
    "                        char = plain_text[i]\n",
    "                        if char == '<':\n",
    "                            flag=0\n",
    "                        elif char == '>':\n",
    "                            flag=1\n",
    "\n",
    "                        elif flag:\n",
    "                            # if char != '\\n':# or text[-1] == '.':\n",
    "                            text += char\n",
    "                    text = \" \".join(text.split())\n",
    "                    doc = nlp(text)\n",
    "                    # out.write(text + '\\n\\n')\n",
    "                    sentences = doc.sents\n",
    "                    flag=0\n",
    "                    prnt_snt = \"\"\n",
    "                    for sent in sentences:\n",
    "                        # out.write(sent + '\\n\\n')\n",
    "                        s1 = sent.lower()\n",
    "                        if ' vs' in s1 or ' v. ' in sent or ' v ' in sent:\n",
    "                            flag = 1\n",
    "                            prnt_snt = sent\n",
    "                        elif flag:\n",
    "                            prnt_snt += sent\n",
    "                            s1 = prnt_snt.lower()\n",
    "                            if 'overruled' in s1:\n",
    "                                out.write(\"Precedent Overruled : \")\n",
    "                            elif 'distinguish' in s1:\n",
    "                                out.write(\"Precedent Distinguished : \")\n",
    "                            elif sia.polarity_scores(prnt_snt)[\"compound\"] > 0:\n",
    "                                out.write(\"Precedent Relied : \")\n",
    "                            else:\n",
    "                                out.write(\"Precedent Referred : \")\n",
    "                                \n",
    "                            out.write(prnt_snt + '\\n\\n')\n",
    "                            flag = 0\n",
    "                            prnt_snt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e6e58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for the directory new_case_json_100\n",
    "import os\n",
    "import json\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "directories = ['new_case_json_100']\n",
    "for directory in directories:\n",
    "    out_dir = directory+'_output'\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".json\", \" output.txt\"))\n",
    "        with open(f, 'r') as file:\n",
    "            with open( out_file, 'w') as out:\n",
    "                data = json.load(file)\n",
    "                text = \"\"\n",
    "                for line in data['JudgmentText']:\n",
    "                    if isinstance(line, str):\n",
    "                        text += line\n",
    "                    elif isinstance(line['I'], str):\n",
    "                        text += line['I']\n",
    "                        # print(filename, type(line['I']), line['I'])\n",
    "                    else:\n",
    "                        text += ''.join(line['I'])\n",
    "                # text = \" \".join(text.split())\n",
    "                # out.write(text + '\\n\\n')\n",
    "                sentences = sent_tokenize(text)\n",
    "                flag=0\n",
    "                prnt_snt = \"\"\n",
    "                for sent in sentences:\n",
    "                    # out.write(sent + '\\n\\n')\n",
    "                    s1 = sent.lower()\n",
    "                    if ' vs' in s1 or ' v. ' in sent or ' v ' in sent:\n",
    "                        flag = 1\n",
    "                        prnt_snt = sent\n",
    "                    elif flag:\n",
    "                        prnt_snt += sent\n",
    "                        s1 = prnt_snt.lower()\n",
    "                        if 'overruled' in s1:\n",
    "                            out.write(\"Precedent Overruled : \")\n",
    "                        elif 'distinguish' in s1:\n",
    "                            out.write(\"Precedent Distinguished : \")\n",
    "                        elif sia.polarity_scores(prnt_snt)[\"compound\"] > 0:\n",
    "                            out.write(\"Precedent Relied : \")\n",
    "                        else:\n",
    "                            out.write(\"Precedent Referred : \")\n",
    "                        out.write(prnt_snt + '\\n\\n')\n",
    "                        flag = 0\n",
    "                        prnt_snt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c54dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for the directory new_case_json_100 with json output\n",
    "import os\n",
    "import json\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "directories = ['new_case_json_100']\n",
    "for directory in directories:\n",
    "    out_dir = directory+'_output'\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".json\", \" output.json\"))\n",
    "        with open(f, 'r') as file:\n",
    "            with open( out_file, 'w') as out:\n",
    "                output = {}\n",
    "                data = json.load(file)\n",
    "                text = \"\"\n",
    "                for line in data['JudgmentText']:\n",
    "                    if isinstance(line, str):\n",
    "                        text += line\n",
    "                    elif isinstance(line['I'], str):\n",
    "                        text += line['I']\n",
    "                        # print(filename, type(line['I']), line['I'])\n",
    "                    else:\n",
    "                        text += ''.join(line['I'])\n",
    "                # text = \" \".join(text.split())\n",
    "                # out.write(text + '\\n\\n')\n",
    "                sentences = sent_tokenize(text)\n",
    "                flag=0\n",
    "                prnt_snt = \"\"\n",
    "                for sent in sentences:\n",
    "                    # out.write(sent + '\\n\\n')\n",
    "                    s1 = sent.lower()\n",
    "                    if ' vs' in s1 or ' v. ' in sent or ' v ' in sent:\n",
    "                        flag = 1\n",
    "                        prnt_snt = sent\n",
    "                    elif flag:\n",
    "                        prnt_snt += sent\n",
    "                        s1 = prnt_snt.lower()\n",
    "                        if 'overruled' in s1:\n",
    "                            # out.write(\"Precedent Overruled : \")\n",
    "                            output[prnt_snt] = \"PRE_OVER\"\n",
    "                        elif 'distinguish' in s1:\n",
    "                            # out.write(\"Precedent Distinguished : \")\n",
    "                            output[prnt_snt] = \"PRE_DIST\"\n",
    "                        elif sia.polarity_scores(prnt_snt)[\"compound\"] > 0:\n",
    "                            # out.write(\"Precedent Relied : \")\n",
    "                            output[prnt_snt] = \"PRE_REL\"\n",
    "                        else:\n",
    "                            # out.write(\"Precedent Referred : \")\n",
    "                            output[prnt_snt] = \"PRE_REF\"\n",
    "                        # out.write(prnt_snt + '\\n\\n')\n",
    "                        flag = 0\n",
    "                        prnt_snt = \"\"\n",
    "\n",
    "                json_output = json.dumps(output, indent=4)\n",
    "                out.write(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0439ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to extract and classify parts of Judgements into XML meta tags\n",
    "import os\n",
    "\n",
    "directories = ['abc']\n",
    "for directory in directories:\n",
    "    out_dir = directory+'_output'\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".txt\", \"_output.txt\"))\n",
    "        with open(f, 'r') as file:\n",
    "            with open( out_file, 'w') as out:\n",
    "                lines = file.readlines()\n",
    "                state=0\n",
    "                for line in lines:\n",
    "                    line1 = line.lower()\n",
    "                    if state == 0 and 'reportable' in line1:\n",
    "                        state = 1\n",
    "                        continue\n",
    "                    if state == 1 and 'court' in line1:\n",
    "                        state = 2\n",
    "                        out.write('Court : ' + line[line1.index('the') + 3:].strip() + '\\n')\n",
    "                        continue\n",
    "                    if state == 2 and 'jurisdiction' in line1:\n",
    "                        state = 3\n",
    "                        # out.write('        ' + line.strip() + '\\n')\n",
    "                        continue\n",
    "                    if (state >= 3 and state < 8) and ('appeal' in line1 or 'petition' in line1) and 'no.' in line1:\n",
    "                        state = max(4, state)\n",
    "                        out.write(\"Case No. : \" + line.strip() + '\\n')\n",
    "                        continue\n",
    "                    if state == 4 and '.appellant' in line1:\n",
    "                        state = 5\n",
    "                        prnt_snt = \"Petitioner : \"\n",
    "                        for word in line.split():\n",
    "                            if 'appellant' in word.lower():\n",
    "                                continue\n",
    "                            prnt_snt += (word + ' ')\n",
    "                        prnt_snt += '\\n'\n",
    "                        out.write(prnt_snt)\n",
    "                        continue\n",
    "                    if state == 5 and 'respondent' in line1:\n",
    "                        state = 6\n",
    "                        prnt_snt = \"Respondent : \"\n",
    "                        for word in line.split():\n",
    "                            if 'respondent' in word.lower():\n",
    "                                continue\n",
    "                            prnt_snt += (word + ' ')\n",
    "                        prnt_snt += '\\n'\n",
    "                        out.write(prnt_snt)\n",
    "                        continue\n",
    "                    if state == 6 and 'judgment' in line1.replace(\" \", \"\"):\n",
    "                        state = 7\n",
    "                        continue\n",
    "                    if state == 7 and len(line) >= 2:\n",
    "                        state = 8\n",
    "                        out.write(\"Judge Name : \" + line.split(',')[0].strip() + '\\n')\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca92bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Code to extract and classify parts of Judgements into XML meta tags\n",
    "import os\n",
    "\n",
    "directories = [\"abc\"]\n",
    "months = {\"january\":\"01\", \"february\":\"02\", \"march\":\"03\", \"april\":\"04\", \"may\":\"05\", \"june\":\"06\", \"july\":\"07\", \"august\":\"08\", \"september\":\"09\", \"october\":\"10\", \"november\":\"11\", \"december\":\"12\"}\n",
    "for directory in directories:\n",
    "    out_dir = directory+'_output_ujp'\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        out_file = os.path.join(out_dir, filename.replace(\".txt\", \"_output.txt\"))\n",
    "        check = {\"Reportable\": False, \"Court\": False, \"Case no\": False, \"Petitioner\": False, \"Respondent\": False, \"Judgement\": False, \"Judge name\": False, \"Date\": False, \"Bench\": False}\n",
    "        if f[-3:] == \"txt\":\n",
    "          with open(f, 'r') as file:\n",
    "              with open( out_file, 'w') as out:\n",
    "                  lines = file.readlines()\n",
    "                  lines = [i.strip() for i in lines]\n",
    "                  bench = []\n",
    "                  while \"\" in lines:\n",
    "                      lines.remove(\"\")\n",
    "                  state=0\n",
    "                  i=0\n",
    "                  for line in lines:\n",
    "                      line1 = line.lower()\n",
    "                      if check[\"Reportable\"] == False and 'reportable' in line1:\n",
    "                          check[\"Reportable\"] = True\n",
    "                          # continue\n",
    "                      if check[\"Reportable\"] == True and check[\"Court\"] == False and 'court' in line1:\n",
    "                          check[\"Court\"] = True\n",
    "                          out.write('Court : ' + line.strip() + '\\n')\n",
    "                          # continue\n",
    "                      if check[\"Reportable\"] == True and check[\"Court\"] == True and 'jurisdiction' in line1:\n",
    "                          # state = 3\n",
    "                          out.write('        ' + line.strip() + '\\n')\n",
    "                          # continue\n",
    "                      if check[\"Case no\"] == False and 'appeal' in line1:\n",
    "                          check[\"Case no\"] = True\n",
    "                          out.write(\"Case No. : \" + line.strip() + '\\n')\n",
    "                          # continue\n",
    "                      if check[\"Petitioner\"] == False and '.appellant' in line1:\n",
    "                          check[\"Petitioner\"] = True\n",
    "                          prnt_snt = \"Petitioner : \"\n",
    "                          for word in line.split():\n",
    "                              if 'appellant' in word.lower():\n",
    "                                  continue\n",
    "                              prnt_snt += (word + ' ')\n",
    "                          prnt_snt += '\\n'\n",
    "                          out.write(prnt_snt)\n",
    "                          # continue\n",
    "                      if check[\"Respondent\"] == False and 'respondent' in line1:\n",
    "                          check[\"Respondent\"] = True\n",
    "                          prnt_snt = \"Respondent : \"\n",
    "                          for word in line.split():\n",
    "                              if 'respondent' in word.lower():\n",
    "                                  continue\n",
    "                              prnt_snt += (word + ' ')\n",
    "                          prnt_snt += '\\n'\n",
    "                          out.write(prnt_snt)\n",
    "                          # continue\n",
    "                      if check[\"Judgement\"]==False and 'judgment' in line1.replace(\" \", \"\"):\n",
    "                          check[\"Judgement\"] = True\n",
    "                          # continue\n",
    "                      if check[\"Judgement\"]==True and check[\"Judge name\"] == False and len(line) >= 2:\n",
    "                          check[\"Judge name\"] = True\n",
    "                          out.write(\"Judge Name : \" + line + \"\\n\")\n",
    "                          # continue\n",
    "                      if check[\"Bench\"]==False:\n",
    "                          if \"[\" in line1 and \"]\" in line1:\n",
    "                              a = line1.find(\"[\")\n",
    "                              b = line1.find(\"]\")\n",
    "                              if \".\" in line1[a+1:b] and \"(\" not in line1[a+1:b]:\n",
    "                                  bench.append(line[a+1:b])\n",
    "                              # print(bench)\n",
    "                          if line==lines[-1]:\n",
    "                              check[\"Bench\"] = True\n",
    "                              out.write(\"Bench : \" + str(bench) + \"\\n\")\n",
    "                      if check[\"Date\"] == False and line==lines[-1]:\n",
    "                          # print(line1.split(\" \"))\n",
    "                          print(f)\n",
    "                          check[\"Date\"] = True\n",
    "                          temp = line1.split(\" \")\n",
    "                        #   print(temp)\n",
    "                          if len(temp)<3:\n",
    "                              continue\n",
    "                          month = temp[0]\n",
    "                          date = temp[1].replace(\",\", \"\")\n",
    "                          year = temp[2].replace(\".\", \"\")\n",
    "                          if month in months:\n",
    "                            month = months[month]\n",
    "                            final_date = date+\"/\"+month+\"/\"+year+\"\\n\"\n",
    "                            # print(final_date)\n",
    "                            out.write(\"Judgement Date : \"+final_date)\n",
    "                            # continue"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
